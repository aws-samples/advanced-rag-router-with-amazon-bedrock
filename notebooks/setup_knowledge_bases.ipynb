{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d6e9f4",
   "metadata": {},
   "source": [
    "# Build an advanced RAG router based assistant with Amazon Bedrock\n",
    "\n",
    "[Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of high-performing Foundation Models (FMs) from leading AI companies accessible through a single API, along with a broad set of capabilities you need to build generative AI applications, simplifying development while maintaining privacy and security.\n",
    "\n",
    "[Large Language Models (LLMs)](https://en.wikipedia.org/wiki/Large_language_model) are a type of Foundation Model that can take natural langauge as input, with the ability to process and understand it, and produce natural language as the output. LLMs can also can perform tasks like classification, summarization, simplification, entity recognition, etc.\n",
    "\n",
    "LLMs are usually trained offline with data that is available until that point of time. As a result, LLMs will not have knowledge of the world after that date. Additionally, LLMs are trained on very general domain corpora, making them less effective for domain-specific tasks. And then, LLMs have the tendency to hallucinate where the model generates text that is incorrect, nonsensical, or not real. Using a [Retrieval Augment Generation (RAG)](https://docs.aws.amazon.com/sagemaker/latest/dg/jumpstart-foundation-models-customize-rag.html) mechanism can help mitigate all these issues. A RAG architecture involves retrieving data that closely matches the text in the user's prompt, from an external datasource, and using it to augment the prompt before sending to the LLM. This prompt augmentation will provide the context that the LLM can use to respond to the prompt.\n",
    "\n",
    "When there are mulitple datasources, there is a need to route the retreival request to the appropriate datasource before performing the actual retrieval. This is a RAG router pattern.\n",
    "\n",
    "This notebook will walk you through the process of building an advanced RAG router based assistant using a Large Language Model (LLM) hosted on [Amazon Bedrock](https://aws.amazon.com/bedrock/) and using [Knowledge Bases for Amazon Bedrock](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base.html) for vectorizing, storing, and retrieving data through semantic search. [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/) will be used as the vector index.\n",
    "\n",
    "We will use [LangChain](https://www.langchain.com/) to simplify the process of constructing the prompts, and interacting with the LLMs and Knowledge Bases (KBs). In the process of working through this notebook, you will learn how to setup the Amazon Bedrock client environment, configure security permissions and use prompt templates in LangChain. Invocations that involve LangChain will be explicitly mentioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3440607b",
   "metadata": {},
   "source": [
    "![](./images/flowchart.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0111293",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>\n",
    "    <ul>\n",
    "        <li>This notebook should only be run from within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html\">Amazon SageMaker Notebook instance</a> or within an <a href=\"https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated.html\">Amazon SageMaker Studio Notebook</a>.</li>\n",
    "        <li>This notebook uses text based models along with their versions that were available at the time of writing. Update these as required.</li>\n",
    "        <li>At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions that has Anthropic Claude 3 and Titan Text Embeddings V2 . Follow the guidance in the <i>Organize imports</i> section of this notebook.</li>\n",
    "        <li>This notebook is recommended to be run with a minimum instance size of <i>ml.t3.medium</i> and\n",
    "            <ul>\n",
    "                <li>With <i>Amazon Linux 2, Jupyter Lab 3</i> as the platform identifier on an Amazon SageMaker Notebook instance.</li>\n",
    "                <li> (or)\n",
    "                <li>With <i>Data Science 3.0</i> as the image on an Amazon SageMaker Studio Notebook.</li>\n",
    "            <ul>\n",
    "        </li>\n",
    "        <li>At the time of this writing, the most relevant latest version of the Kernel for running this notebook,\n",
    "            <ul>\n",
    "                <li>On an Amazon SageMaker Notebook instance was <i>conda_python3</i></li>\n",
    "                <li>On an Amazon SageMaker Studio Notebook was <i>Python 3</i></li>\n",
    "            </ul>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e88d20",
   "metadata": {},
   "source": [
    "**Table of Contents:**\n",
    "\n",
    "1. [Complete prerequisites](#Complete%20prerequisites)\n",
    "\n",
    "    a. [Check and configure access to the Internet](#Check%20and%20configure%20access%20to%20the%20Internet)\n",
    "\n",
    "    b. [Install required software libraries](#Install%20required%20software%20libraries)\n",
    "    \n",
    "    c. [Configure logging](#Configure%20logging)\n",
    "    \n",
    "    d. [Organize imports](#Organize%20imports)\n",
    "    \n",
    "    e. [Set AWS Region and boto3 config](#Set%20AWS%20Region%20and%20boto3%20config)\n",
    "    \n",
    "    f. [Enable model access in Amazon Bedrock](#Enable%20model%20access%20in%20Amazon%20Bedrock)\n",
    "    \n",
    "    g. [Check and configure security permissions](#Check%20and%20configure%20security%20permissions)\n",
    "    \n",
    "    h. [Create common objects](#Create%20common%20objects)\n",
    "    \n",
    "    i. [Get details of Knowledge Bases](#Get%20details%20of%20Knowledge%20Bases)\n",
    "\n",
    " 2. [Load data to Knowledge Bases](#Load%20data%20to%20Knowledge%20Bases)\n",
    "    \n",
    "    a. [Step0a: Copy provided data to S3](#Load%20to%20KB%20Step0b)\n",
    "    \n",
    "    b. [Steps 0b to 0e: Sync to Knowledge Base](#Load%20to%20KB%20Steps0c%20to%200e)\n",
    " \n",
    " 3. [Process query](#Process%20query)\n",
    " \n",
    "     a. [Step 1: User query and non-KB query](#User%20query)\n",
    "     \n",
    "     b. [Steps 2a and 2b: Determine the KB id](#Determine%20the%20KB%20id)\n",
    "     \n",
    "     c. [Steps 3a through 5: Retrieve and generate](#Retrieve%20and%20generate)\n",
    " \n",
    " 4. [Conclusion](#Conclusion)\n",
    " \n",
    " 5. [Frequently Asked Questions (FAQs)](#FAQs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fb9d3",
   "metadata": {},
   "source": [
    "##  1. Complete prerequisites <a id ='Complete%20prerequisites'> </a>\n",
    "\n",
    "Check and complete the prerequisites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e85c39b",
   "metadata": {},
   "source": [
    "###  A. Check and configure access to the Internet <a id ='Check%20and%20configure%20access%20to%20the%20Internet'> </a>\n",
    "This notebook requires outbound access to the Internet to download the required software updates and to download the dataset.  You can either provide direct Internet access (default) or provide Internet access through an [Amazon VPC](https://aws.amazon.com/vpc/).  For more information on this, refer [here](https://docs.aws.amazon.com/sagemaker/latest/dg/appendix-notebook-and-internet-access.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820efd56",
   "metadata": {},
   "source": [
    "### B. Install required software libraries <a id ='Install%20required%20software%20libraries'> </a>\n",
    "This notebook requires the following libraries:\n",
    "* [SageMaker Python SDK version 2.x](https://sagemaker.readthedocs.io/en/stable/v2.html)\n",
    "* [Python 3.10.x](https://www.python.org/downloads/release/python-3100/)\n",
    "* [Boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)\n",
    "* [LangChain](https://www.langchain.com/)\n",
    "\n",
    "Run the following cell to install the required libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb373af",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "    <b>Note:</b> At the end of the installation, the Kernel will be forcefully restarted immediately. Please wait 10 seconds for the kernel to come back before running the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7256d4fc-0361-4cee-a548-d9b7e355824f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install boto3==1.35.43\n",
    "!pip install langchain==0.3.4\n",
    "!pip install langchain-aws==0.2.2\n",
    "!pip install langchain-community==0.3.3\n",
    "!pip install requests==2.32.3\n",
    "!pip install sagemaker==2.232.2\n",
    "!pip install ragas\n",
    "\n",
    "\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b3c44f",
   "metadata": {},
   "source": [
    "### C. Configure logging <a id ='Configure%20logging'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd5ee37",
   "metadata": {},
   "source": [
    "####  a. System logs (Optional) <a id='Configure%20system%20logs%20(Optional)'></a>\n",
    "\n",
    "System logs refers to the logs generated by the notebook's interactions with the underlying notebook instance. Some examples of these are the logs generated when loading or saving the notebook.\n",
    "\n",
    "These logs are automatically setup when the notebook instance is launched.\n",
    "\n",
    "These logs can be accessed through the [Amazon CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html) console in the same AWS Region where this notebook is running.\n",
    "* When running this notebook in an Amazon SageMaker Notebook instance, navigate to the following location,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/NotebookInstances > {notebook-instance-name}/jupyter.log</i>\n",
    "* When running this notebook in an Amazon SageMaker Studio Notebook, navigate to the following locations,\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/KernelGateway/{notebook-instance-name}</i>\n",
    "    * <i>CloudWatch > Log groups > /aws/sagemaker/studio > {sagmaker-domain-name}/{user-name}/JupyterServer/default</i>\n",
    "\n",
    "If you want to find out the name of the underlying instance where this notebook is running, uncomment the following code cell and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c99c519",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import json\n",
    "\n",
    "notebook_name = ''\n",
    "resource_metadata_path = '/opt/ml/metadata/resource-metadata.json'\n",
    "with open(resource_metadata_path, 'r') as metadata:\n",
    "    notebook_name = (json.load(metadata))['ResourceName']\n",
    "print(\"Notebook instance name: '{}'\".format(notebook_name))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cc0025",
   "metadata": {},
   "source": [
    "####  b. Application logs <a id='Configure%20application%20logs'></a>\n",
    "\n",
    "Application logs refers to the logs generated by running the various code cells in this notebook. To set this up, instantiate the [Python logging service](https://docs.python.org/3/library/logging.html) by running the following cell. You can configure the default log level and format as required.\n",
    "\n",
    "By default, this notebook will only print the logs to the corresponding cell's output console."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf96e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the logging level and format\n",
    "log_level = logging.INFO\n",
    "log_format = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(level=log_level, format=log_format)\n",
    "\n",
    "# Save these in the environment variables for use in the helper scripts\n",
    "os.environ['LOG_LEVEL'] = str(log_level)\n",
    "os.environ['LOG_FORMAT'] = log_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3bb063",
   "metadata": {},
   "source": [
    "###  D. Organize imports <a id ='Organize%20imports'> </a>\n",
    "\n",
    "Organize all the library and module imports for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764a06b9-812c-4dad-a652-1cb34aa9d8b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import langchain\n",
    "import sagemaker\n",
    "import sys\n",
    "import time\n",
    "import pprint as pp\n",
    "from botocore.config import Config\n",
    "\n",
    "# Import the helper functions from the 'scripts' folder\n",
    "sys.path.append(os.path.join(os.getcwd(), \"scripts\"))\n",
    "#logging.info(\"Updated sys.path: {}\".format(sys.path))\n",
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48ba26b",
   "metadata": {},
   "source": [
    "Print the installed versions of some of the important libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb23f2d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logging.info(\"Python version : {}\".format(sys.version))\n",
    "logging.info(\"Boto3 version : {}\".format(boto3.__version__))\n",
    "logging.info(\"SageMaker Python SDK version : {}\".format(sagemaker.__version__))\n",
    "logging.info(\"LangChain version : {}\".format(langchain.__version__))\n",
    "logging.info(\"Requests version : {}\".format(requests.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f63309",
   "metadata": {},
   "source": [
    "###  E. Set AWS Region and boto3 config <a id ='Set%20AWS%20Region%20and%20boto3%20config'> </a>\n",
    "\n",
    "Get the current AWS Region (where this notebook is running) and the SageMaker Session. These will be used to initialize some of the clients to AWS services using the boto3 APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746eb15",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<b>Note:</b> All the AWS services used by this notebook except Amazon Bedrock will use the current AWS Region. For Bedrock, follow the guidance in the next cell.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de7be30",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> At the time of writing this notebook, Amazon Bedrock was only available in <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-regions.html\">these supported AWS Regions</a>. If you are running this notebook from any other AWS Region, then you have to change the Amazon Bedrock client's region and/or endpoint URL parameters to one of those supported AWS Regions that has Anthropic Claude 3 and Titan Text Embeddings V2 . In order to do this, this notebook will use the value specified in the environment variable named <mark>AMAZON_BEDROCK_REGION</mark>. If this is not specified, then the notebook will default to <mark>us-west-2 (Oregon)</mark> for Amazon Bedrock.\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a6cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the AWS Region, SageMaker Session and IAM Role references\n",
    "my_session = boto3.session.Session()\n",
    "logging.info(\"SageMaker Session: {}\".format(my_session))\n",
    "my_iam_role = sagemaker.get_execution_role()\n",
    "logging.info(\"Notebook IAM Role: {}\".format(my_iam_role))\n",
    "my_region = my_session.region_name\n",
    "logging.info(\"Current AWS Region: {}\".format(my_region))\n",
    "\n",
    "# Explicity set the AWS Region for Amazon Bedrock clients\n",
    "AMAZON_BEDROCK_DEFAULT_REGION = \"us-west-2\"\n",
    "br_region = os.environ.get('AMAZON_BEDROCK_REGION')\n",
    "if br_region is None:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "elif len(br_region) == 0:\n",
    "    br_region = AMAZON_BEDROCK_DEFAULT_REGION\n",
    "logging.info(\"AWS Region for Amazon Bedrock: {}\".format(br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8484fc",
   "metadata": {},
   "source": [
    "Set the timeout and retry configurations that will be applied to all the boto3 clients used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "037155d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the standard time out limits in the boto3 client from 1 minute to 3 minutes\n",
    "# and set the retry limits\n",
    "my_boto3_config = Config(\n",
    "    connect_timeout = (60 * 3),\n",
    "    read_timeout = (60 * 3),\n",
    "    retries = {\n",
    "        'max_attempts': 10,\n",
    "        'mode': 'standard'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984f0e97",
   "metadata": {},
   "source": [
    "###  F. Enable model access in Amazon Bedrock <a id ='Enable%20model%20access%20in%20Amazon%20Bedrock'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e84ba9d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> Before proceeding further with this notebook, you must enable access to the 'Anthropic Claude 3' and 'Titan Text Embeddings V2' models on Amazon Bedrock by following the instructions <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">here</a>. You need to submit the use case details. Otherwise, you will get an authorization error.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57899f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">  \n",
    "<b>Note:</b> You will have to do this manually after reading the End User License Agreement (EULA) for each of the models that you want to enable. Unless you explicitly disable it, this is a one-time setup for each model in an AWS account.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19de160b",
   "metadata": {},
   "source": [
    "Run the following cell to print the Amazon Bedrock model access page URL for the AWS Region that was selected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Amazon Bedrock model access page URL\n",
    "logging.info(\"Amazon Bedrock model access page - https://{}.console.aws.amazon.com/bedrock/home?region={}#/modelaccess\"\n",
    "             .format(br_region, br_region))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c09b6c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">  \n",
    "<b>Note:</b> For running this notebook, you need access to only the Anthropic Claude 3 Haiku and Sonnet models, and the Titan Text Embeddings V2  model. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2ee077",
   "metadata": {},
   "source": [
    "###  G. Check and configure security permissions <a id ='Check%20and%20configure%20security%20permissions'> </a>\n",
    "This notebook uses the IAM role attached to the underlying notebook instance.  To view the name of this role, run the following cell. This IAM role should have the following permissions,\n",
    "1. Full access to invoke Large Language Models (LLMs) on Amazon Bedrock.\n",
    "2. Full access to use Knowledge Bases for Amazon Bedrock.\n",
    "3. Full access to read and write to the Amazon OpenSearch Serverless collection associated with the Knowledge Base.\n",
    "4. Full access to read and write to the Amazon S3 bucket associated with the Knowledge Base.\n",
    "5. Access to write to Amazon CloudWatch Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688f610",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b>  If you are running this notebook as part of a workshop session, by default, all these permissions will be setup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16baab7",
   "metadata": {},
   "source": [
    "Run the following cell to print the details of the IAM role attached to the underlying notebook instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c64186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the IAM role ARN and console URL\n",
    "logging.info(\"This notebook's IAM role is '{}'\".format(my_iam_role))\n",
    "arn_parts = my_iam_role.split('/')\n",
    "logging.info(\"Details of this IAM role are available at https://{}.console.aws.amazon.com/iamv2/home?region={}#/roles/details/{}?section=permissions\"\n",
    "             .format(my_region, my_region, arn_parts[len(arn_parts) - 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12579ad7",
   "metadata": {},
   "source": [
    "###  H. Create common objects <a id='Create%20common%20objects'></a>\n",
    "\n",
    "To begin, let's create the boto3 clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b256c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Amazon S3 client\n",
    "s3_client = boto3.client(\"s3\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Create the Amazon OpenSearch Serverless client\n",
    "aoss_client = boto3.client(\"opensearchserverless\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Create the Amazon Bedrock client\n",
    "bedrock_client = boto3.client(\"bedrock\", region_name = br_region, endpoint_url = \"https://bedrock.{}.amazonaws.com\"\n",
    "                              .format(br_region), config = my_boto3_config)\n",
    "\n",
    "# Create the Amazon Bedrock runtime client\n",
    "bedrock_rt_client = boto3.client(\"bedrock-runtime\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Create the Agents for Amazon Bedrock client\n",
    "bedrock_agt_client = boto3.client(\"bedrock-agent\", region_name = br_region, config = my_boto3_config)\n",
    "\n",
    "# Create the Agents for Amazon Bedrock runtime client\n",
    "bedrock_agt_rt_client = boto3.client(\"bedrock-agent-runtime\", region_name = br_region, config = my_boto3_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900c771e",
   "metadata": {},
   "source": [
    "List all Anthropic Claude 3 LLMs on Amazon Bedrock that are offered through the On-Demand throughput pricing model. This will help you pick the model-ids that you will use further down in this notebook.\n",
    "\n",
    "For more information on this, refer [here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323e08e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note: 'print_claude_3_llm_info' available through ./scripts/helper_functions.py\n",
    "print_claude_3_llm_info(bedrock_client, 'ON_DEMAND')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b1322f",
   "metadata": {},
   "source": [
    "Create the common objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693def8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the Anthropic Claude 3 model-id\n",
    "#model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "# Specify the URL of the data to be used for RAG\n",
    "kb_1_category = \"Amazon Bedrock\"\n",
    "kb_2_category = \"Code\"\n",
    "\n",
    "# Specify the Amazon S3 bucket key prefix\n",
    "# Note: The bucket name will be automatically determined\n",
    "# from the Knowledge Base data source when executing cells\n",
    "# further down in this notebook\n",
    "s3_key_prefix = \"rag_data\"\n",
    "\n",
    "# Specify the name and location of the prompt templates\n",
    "prompt_templates_dir = os.path.join(os.getcwd(), \"prompt_templates\")\n",
    "kb_router_system_prompt_template = 'kb_router_system_prompt_template.txt'\n",
    "kb_router_user_prompt_template = 'kb_router_user_prompt_template.txt'\n",
    "final_system_prompt_template = 'final_system_prompt_template.txt'\n",
    "final_user_prompt_template = 'final_user_prompt_template.txt'\n",
    "\n",
    "# Specify and create the required output directories\n",
    "kb_1_rag_data_dir = os.path.join(os.getcwd(), \"kb_1_rag_data\")\n",
    "kb_2_rag_data_dir = os.path.join(os.getcwd(), \"kb_2_rag_data\")\n",
    "os.makedirs(kb_1_rag_data_dir, exist_ok = True)\n",
    "os.makedirs(kb_2_rag_data_dir, exist_ok = True)\n",
    "\n",
    "# Specify the maximum number of query results to retrieve from the KBs\n",
    "max_query_results = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb0a620",
   "metadata": {},
   "source": [
    "###  I. Get details of Knowledge Bases <a id='Get%20details%20of%20Knowledge%20Bases'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ab613",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> For the purpose of running this notebook, two new Knowledge Bases (KBs) must be created in the same AWS Region as Amazon Bedrock that was configured in Step 1E of this notebook.\n",
    "<p>These KBs must meet the following requirements:\n",
    "    <ul>\n",
    "        <li>Must be in 'ACTIVE' status.</li>\n",
    "        <li>Must have an Amazon OpenSearch Serverless collection as the vector index.</li>\n",
    "        <li>Must have an Amazon S3 bucket as the data source.</li>\n",
    "        <li>Data source must be in 'AVAILABLE' status.</li>\n",
    "        <li>Embeddings model must be 'Titan Text Embeddings V2 '.</li>\n",
    "    </ul>\n",
    "</p>\n",
    "<p>If you are running this notebook as part of a workshop session, by default, two KBs that meet all these requirements will be pre-created and ready to use.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a1ddc4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    <b>Note:</b> If you are running this notebook outside of a workshop session, then, you must create two KBs as specified above. Otherwise, this notebook will fail. You can follow the procedure described <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-create.html\">here</a> to create a KB.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62546f06",
   "metadata": {},
   "source": [
    "Specify the names of the KBs to be used in this notebook by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7367d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_1_name = 'rag-router-kb-1'\n",
    "kb_2_name = 'rag-router-kb-2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e726d81",
   "metadata": {},
   "source": [
    "Run the following cell to verify if the specified KBs exist and if they meet all the requirements. In the process, retreive the S3 bucket names that are set as the data sources for these KBs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66348062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'get_kb_details' available through ./scripts/helper_functions.py\n",
    "kb_1_id, kb_1_ds_id, kb_1_s3_bucket_name, kb_1_aoss_collection_arn = get_kb_details(bedrock_agt_client, kb_1_name)\n",
    "kb_2_id, kb_2_ds_id, kb_2_s3_bucket_name, kb_2_aoss_collection_arn = get_kb_details(bedrock_agt_client, kb_2_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a8504",
   "metadata": {},
   "source": [
    "## 2. Load data to Knowledge Bases <a id ='Load%20data%20to%20Knowledge%20Bases'> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328ad255",
   "metadata": {},
   "source": [
    "![](./images/load_to_KBs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a96329",
   "metadata": {},
   "source": [
    "###  B. Step 0a: Copy provided data to S3 <a id='Load%20to%20KB%20Step0b'></a>\n",
    "\n",
    "Copy the provided files to the corresponding Amazon S3 buckets. These buckets will be the [data sources](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-ds.html) for the Knowledge Bases (KBs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70117449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'upload_to_s3' available through ./scripts/helper_functions.py\n",
    "upload_to_s3(\"pdf_docs\", kb_1_s3_bucket_name, s3_key_prefix)\n",
    "upload_to_s3(\"blogs\", kb_2_s3_bucket_name, s3_key_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84da7f3f",
   "metadata": {},
   "source": [
    "###  C. Step 0c Create web data sources<a id='Load%20to%20KB%20Steps0c%20to%200e'></a>\n",
    "\n",
    "Trigger the [sync operation](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-ingest.html) on the Knowledge Bases to load the files from that data sources (S3 buckets) to the [vector index](https://docs.aws.amazon.com/bedrock/latest/userguide/knowledge-base-setup.html) configured for in those Knowledge Bases (KBs). In the process of loading, the data will be chunked and converted to vectors using the Embeddings Model specified for those KBs. \n",
    "\n",
    "We are going to create two separate data sources. One with fixed size chunking and one with hierarchical chunking. Hierarchical chunking organizes chunks into parent and child chunks that aligns with the structure of your input document. We will use this for the Bedrock user guide, which we are indexing. To learn more about chunking, look at our documentation [here](https://docs.aws.amazon.com/bedrock/latest/userguide/kb-chunking-parsing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bebf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources_kb1 = [\n",
    "{\"type\": \"S3\", \"bucket_name\": kb_1_s3_bucket_name, \"chunking_strategy\":\"HIERARCHICAL\"}, \n",
    "]\n",
    "data_sources_kb2 = [\n",
    "{\"type\": \"S3\", \"bucket_name\": kb_2_s3_bucket_name, \"chunking_strategy\":\"FIXED_SIZE\"},\n",
    "]\n",
    "# to create a semantic chunking db, use the following:\n",
    "# {\"type\": \"S3\", \"bucket_name\": kb_3_s3_bucket_name, \"chunking_strategy\":\"SEMANTIC\"},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52edb7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create KB\n",
    "def create_ds(data_sources, kb_id, name = \"web-data-source\", description = \"aws blog web data source\"):\n",
    "    ds_list=[]\n",
    "    for idx, ds in enumerate(data_sources):\n",
    "        # Ingest strategy - How to ingest data from the data source\n",
    "        if ds['chunking_strategy'] == \"FIXED_SIZE\":\n",
    "            chunkingStrategyConfiguration = {\n",
    "                \"chunkingStrategy\": \"FIXED_SIZE\", \n",
    "                \"fixedSizeChunkingConfiguration\": {\n",
    "                    \"maxTokens\": 512,\n",
    "                    \"overlapPercentage\": 20\n",
    "                }\n",
    "            }\n",
    "        elif ds['chunking_strategy'] == \"HIERARCHICAL\":\n",
    "            chunkingStrategyConfiguration = {\n",
    "                \"chunkingStrategy\": \"HIERARCHICAL\", \n",
    "                \"hierarchicalChunkingConfiguration\": {\n",
    "                    \"overlapTokens\": 100,\n",
    "                    \"levelConfigurations\":[\n",
    "                    {\n",
    "                        \"maxTokens\":512\n",
    "                    },\n",
    "                    {\n",
    "                        \"maxTokens\":256\n",
    "                    }]\n",
    "                    \n",
    "                }\n",
    "            }\n",
    "        elif ds['chunking_strategy'] == \"SEMANTIC\":\n",
    "            chunkingStrategyConfiguration = {\n",
    "                \"chunkingStrategy\": \"SEMANTIC\", \n",
    "            }\n",
    "        \n",
    "        # The data source to ingest documents from, into the OpenSearch serverless knowledge base index\n",
    "        \n",
    "        s3DataSourceConfiguration = {\n",
    "                \"type\": \"S3\",\n",
    "                \"s3Configuration\":{\n",
    "                    \"bucketArn\": \"\",\n",
    "                    # \"inclusionPrefixes\":[\"*.*\"] # you can use this if you want to create a KB using data within s3 prefixes.\n",
    "                    }\n",
    "            }\n",
    "        \n",
    "\n",
    "        webcrawlerDataSourceConfiguration = {\n",
    "            \"webConfiguration\": {\n",
    "                \"sourceConfiguration\": {\n",
    "                    \"urlConfiguration\": {\n",
    "                        \"seedUrls\": []\n",
    "                    }\n",
    "                },\n",
    "                \"crawlerConfiguration\": {\n",
    "                    \"crawlerLimits\": {\n",
    "                        \"rateLimit\": 50\n",
    "                    },\n",
    "                    \"scope\": \"HOST_ONLY\",\n",
    "                    \"inclusionFilters\": [],\n",
    "                    \"exclusionFilters\": []\n",
    "                }\n",
    "            },\n",
    "            \"type\": \"WEB\"\n",
    "        }\n",
    "\n",
    "        # Set the data source configuration based on the Data source type\n",
    "\n",
    "        if ds['type'] == \"S3\":\n",
    "            print(f'{idx +1 } data source: S3')\n",
    "            ds_name = f'{name}-{ds[\"bucket_name\"]}'\n",
    "            s3DataSourceConfiguration[\"s3Configuration\"][\"bucketArn\"] = f'arn:aws:s3:::{ds[\"bucket_name\"]}'\n",
    "            # print(s3DataSourceConfiguration)\n",
    "            data_source_configuration = s3DataSourceConfiguration\n",
    "        \n",
    "        if ds['type'] == \"WEB\":\n",
    "            print(f'{idx +1 } data source: WEB')\n",
    "            ds_name = f'{name}-web'\n",
    "            webcrawlerDataSourceConfiguration['webConfiguration']['sourceConfiguration']['urlConfiguration']['seedUrls'] = ds['seedUrls']\n",
    "            webcrawlerDataSourceConfiguration['webConfiguration']['crawlerConfiguration']['inclusionFilters'] = ds['inclusionFilters']\n",
    "            webcrawlerDataSourceConfiguration['webConfiguration']['crawlerConfiguration']['exclusionFilters'] = ds['exclusionFilters']\n",
    "            # print(webcrawlerDataSourceConfiguration)\n",
    "            data_source_configuration = webcrawlerDataSourceConfiguration\n",
    "            \n",
    "\n",
    "        # Create a DataSource in KnowledgeBase \n",
    "        create_ds_response = bedrock_agt_client.create_data_source(\n",
    "            name = ds_name,\n",
    "            description = description,\n",
    "            knowledgeBaseId = kb_id,\n",
    "            dataSourceConfiguration = data_source_configuration,\n",
    "            vectorIngestionConfiguration = {\n",
    "                \"chunkingConfiguration\": chunkingStrategyConfiguration\n",
    "            }\n",
    "        )\n",
    "        ds = create_ds_response[\"dataSource\"]\n",
    "        pp.pprint(ds)\n",
    "        ds_list.append(ds)\n",
    "    return ds_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2453d22",
   "metadata": {},
   "source": [
    "Create our data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab9b1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list1 = create_ds(data_sources_kb1, kb_1_id, name = \"bedrock-user-guide\", description = \"Amazon Bedrock user guide\")\n",
    "ds_list2 = create_ds(data_sources_kb2, kb_2_id, name = \"aws-blogs\", description = \"aws blog data source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773dfda0",
   "metadata": {},
   "source": [
    "###  D. Step 0d: Sync to Knowledge Base <a id='Load%20to%20KB%20Step0b'></a>\n",
    "\n",
    "Let's trigger our sync operations. We will check the status of the ingestion every 5 seconds until failed or complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d507a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: 'sync_to_kb' available through ./scripts/helper_functions.py\n",
    "sync_to_kb(bedrock_agt_client, ds_list1[0]['dataSourceId'], kb_1_id, 'Sync PDF from S3 to vector index')\n",
    "sync_to_kb(bedrock_agt_client,  ds_list2[0]['dataSourceId'], kb_2_id, 'sync AWS blogs to vector index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7efe68",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> Even if the sync operation is in 'COMPLETE' status, it may take up to 60 seconds for the data to be available for reading from the vector index.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd2c32",
   "metadata": {},
   "source": [
    "## 3. Process query <a id ='Process%20query'> </a>\n",
    "\n",
    "Based on the specified query, determine the right Knowledge Base (KB) to retrieve from, perform the retrieval and generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36960046",
   "metadata": {},
   "source": [
    "![](./images/retrieve_and_generate.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e027e",
   "metadata": {},
   "source": [
    "###  A. Step 1a and 1b: User query and non-KB response <a id='User%20query'></a>\n",
    "\n",
    "Let's set the user query and try to answer it without a knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a762173b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category: Code Samples\n",
    "#query = 'Get me code to create a new KB'\n",
    "\n",
    "# Category: Amazon Bedrock\n",
    "query = 'What is a Bedrock KB?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7910e903-c0d8-4c67-9941-32e01a1ef2f9",
   "metadata": {},
   "source": [
    "Let's try to answer this question without our knowledge base. Here we will use the [Amazon Bedrock Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html#message-inference-examples). The converse API provides an easy way for us to converse with LLMs hosted in Bedrock. Let's ask Claude Sonnet about our question.\n",
    "\n",
    "To learn more about prompting guidelines, you can find information [here](https://docs.aws.amazon.com/bedrock/latest/userguide/general-guidelines-for-bedrock-users.html#:~:text=Temperature%20is%20a%20value%20between,from%20LLMs%20on%20Amazon%20Bedrock.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba1e382-928e-4e3b-b886-77c53d878868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature is a value between 0 and 1, it regulates the creativity of LLMs' responses.\n",
    "temperature = 0\n",
    "# Top-k sampling refines the selection process by limiting the pool of potential next tokens to the top-k most likely candidates.\n",
    "top_k = 200\n",
    "\n",
    "# Base inference parameters to use.\n",
    "inference_config = {\"temperature\": temperature}\n",
    "# Additional inference parameters to use.\n",
    "additional_model_fields = {\"top_k\": top_k}\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": query}]\n",
    "}]\n",
    "# Setup the system prompts and messages to send to the model. System prompts can be used to give your model a specific persona or set other context.\n",
    "system_prompts = [{\"text\": \"You are an aws expert\"}]\n",
    "\n",
    "response = bedrock_rt_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "logging.info(response[\"output\"][\"message\"][\"content\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08c7e7-ce74-4fce-afbb-cca35954196d",
   "metadata": {},
   "source": [
    "Let's try again, but this time using our knowledge base. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12ded7",
   "metadata": {},
   "source": [
    "###  B. Steps 2a and 2b: Determine the KB id <a id='Determine%20the%20KB%20id'></a>\n",
    "\n",
    "Determine the Knowledge Base (KB) id to retrieve from by prompting the LLM with the query and the list of categories and KB ids."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a4afc7-aa06-4e6b-8421-784b54bc0c47",
   "metadata": {},
   "source": [
    "Let's take a look at the prompt we are using. The basics structure is\n",
    "- we specify the available knowledge bases and the category they are associated with\n",
    "- the output format we are looking for\n",
    "- explicit instructions on how to evaluate the user query.\n",
    "\n",
    "Our output from the model then is a specific KB ID we can then query to get context for our next LLM call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e95c3-f124-43d0-b63e-e141e2851ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our prompt and print it out.\n",
    "router_prompt_template_file_path = os.path.join(prompt_templates_dir, kb_router_user_prompt_template)\n",
    "router_prompt_template = PromptTemplate.from_file(router_prompt_template_file_path)\n",
    "logging.info(router_prompt_template.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fbb02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the LLM to determin the KB id\n",
    "temperature = 0\n",
    "\n",
    "# Base inference parameters to use.\n",
    "inference_config = {\"temperature\": temperature}\n",
    "\n",
    "# filled prompt template\n",
    "filled_router_template = router_prompt_template.format(QUERY=query, \n",
    "                                         CATEGORY_1=\"code\", \n",
    "                                         CATEGORY_2=\"docs\")\n",
    "\n",
    "router_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": filled_router_template}]\n",
    "}]\n",
    "# Setup the system prompts and messages to send to the model. System prompts can be used to give your model a specific persona or set other context.\n",
    "system_prompts = [{\"text\": \"You are an aws expert\"}]\n",
    "\n",
    "response = bedrock_rt_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=router_messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "query_category = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "#small dictionary mapping kb ids to the categories \n",
    "category_dict = {\"code\":kb_1_id,\n",
    " \"docs\":kb_2_id}\n",
    "\n",
    "query_kb_id = category_dict[query_category]\n",
    "logging.info(query_category)\n",
    "\n",
    "# Process the response\n",
    "if query_category == 'UNKNOWN':\n",
    "    logging.error(\"The LLM was unable to identify the category of the specified query from the list of provided categories.\")\n",
    "else:\n",
    "    logging.info(\"The LLM was able to identify the category of the specified query from the list of provided categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ea70d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the output\n",
    "logging.info(\"\\n\\nQUERY: {}\\n\\nCATEGORY: {}\".format(query, query_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125920a",
   "metadata": {},
   "source": [
    "###  C. Steps 3a through 5: Retrieve and generate <a id='Retrieve%20and%20generate'></a>\n",
    "\n",
    "Perform a semantic search on the vector store in the Knowledge Base (KB) for the user query and retrieve the results. The following cell shows you two ways to do this - one using the LangChain API and the other using the boto3 API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b93407",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the LangChain API\n",
    "# Note: 'retrieve_from_kb_using_lc' available through ./scripts/helper_functions.py\n",
    "query_lookup_results = retrieve_from_kb_using_lc(bedrock_agt_rt_client, query_kb_id, query, max_query_results)\n",
    "\n",
    "# Use the boto3 API\n",
    "# Note: 'retrieve_from_kb_using_boto3' available through ./scripts/helper_functions.py\n",
    "#query_lookup_results = retrieve_from_kb_using_boto3(bedrock_agt_rt_client, query_kb_id, query, max_query_results)\n",
    "\n",
    "# Print the retrieval count\n",
    "logging.info(\"Retrieved {} result(s) for the specified query '{}' from the Knowledge Base '{}'.\".format(len(query_lookup_results),\n",
    "                                                                                                        query,\n",
    "                                                                                                        query_kb_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df80cc9",
   "metadata": {},
   "source": [
    "Construct the prompt, invoke the LLM and generate the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c67a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get our prompt and print it out.\n",
    "final_prompt_template_file_path = os.path.join(prompt_templates_dir, final_user_prompt_template)\n",
    "final_prompt_template = PromptTemplate.from_file(final_prompt_template_file_path)\n",
    "logging.info(final_prompt_template.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b15b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke the LLM to determine the KB id\n",
    "temperature = 0\n",
    "\n",
    "# Base inference parameters to use.\n",
    "inference_config = {\"temperature\": temperature}\n",
    "\n",
    "# filled prompt template\n",
    "filled_final_template = final_prompt_template.format(QUERY=query, \n",
    "                                         CONTEXT=query_lookup_results, \n",
    "                                        )\n",
    "\n",
    "final_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": filled_final_template}]\n",
    "}]\n",
    "# Setup the system prompts and messages to send to the model. System prompts can be used to give your model a specific persona or set other context.\n",
    "system_prompts = [{\"text\": \"You are an aws expert\"}]\n",
    "\n",
    "response = bedrock_rt_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=final_messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "query_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "logging.info(\"\\n\\nQUERY: {}\\n\\nRESPONSE: {}\".format(query, query_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcec0cc-aa1b-401c-88c4-98923879df5e",
   "metadata": {},
   "source": [
    "Let's try another query, but this time we will ask a question that will be routed to a different KB. Let's run it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c45231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category: Large Language Models\n",
    "query = 'How can I create a new knowledge base using boto3?'\n",
    "\n",
    "# filled prompt template\n",
    "filled_router_template = router_prompt_template.format(QUERY=query, \n",
    "                                         CATEGORY_1=\"code\", \n",
    "                                         CATEGORY_2=\"docs\")\n",
    "\n",
    "router_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": filled_router_template}]\n",
    "}]\n",
    "# Setup the system prompts and messages to send to the model. System prompts can be used to give your model a specific persona or set other context.\n",
    "system_prompts = [{\"text\": \"You are an aws expert\"}]\n",
    "\n",
    "# run router LLM call\n",
    "response = bedrock_rt_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=router_messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "query_category = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "query_kb_id = category_dict[query_category]\n",
    "logging.info(query_category)\n",
    "\n",
    "# retrieve results from our knowledge base\n",
    "query_lookup_results = retrieve_from_kb_using_lc(bedrock_agt_rt_client, query_kb_id, query, max_query_results)\n",
    "\n",
    "# filled prompt template\n",
    "filled_final_template = final_prompt_template.format(QUERY=query, \n",
    "                                         CONTEXT=query_lookup_results, \n",
    "                                        )\n",
    "\n",
    "final_messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": filled_final_template}]\n",
    "}]\n",
    "\n",
    "# execute final prompt\n",
    "response = bedrock_rt_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=final_messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "query_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "logging.info(\"\\n\\nQUERY: {}\\n\\nRESPONSE: {}\".format(query, query_response))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bd52a5",
   "metadata": {},
   "source": [
    "## 4. Conclusion <a id='Conclusion'></a>\n",
    "\n",
    "We have now seen how to build an advanced RAG router based assistant with Amazon Bedrock. Now you can move on to the next notebook `prompt_flow_generation_aws.ipynb` to see how to create a prompt flow application for your chat assistant!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf266cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## 5. Frequently Asked Questions (FAQs) <a id='FAQs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2c50f",
   "metadata": {},
   "source": [
    "**Q: What AWS services are used in this notebook?**\n",
    "\n",
    "Amazon Bedrock, Amazon OpenSearch Serverless, Amazon S3, AWS Identity and Access Management (IAM), Amazon CloudWatch, and Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook depending on what you use to run the notebook.\n",
    "\n",
    "**Q: Will Amazon Bedrock capture and store my data?**\n",
    "\n",
    "Amazon Bedrock doesn't use your prompts and continuations to train any AWS models or distribute them to third parties. Your training data isn't used to train the base Amazon Titan models or distributed to third parties. Other usage data, such as usage timestamps, logged account IDs, and other information logged by the service, is also not used to train the models.\n",
    "\n",
    "Amazon Bedrock uses the fine tuning data you provide only for fine tuning an Amazon Titan model. Amazon Bedrock doesn't use fine tuning data for any other purpose, such as training base foundation models.\n",
    "\n",
    "Each model provider has an escrow account that they upload their models to. The Amazon Bedrock inference account has permissions to call these models, but the escrow accounts themselves don't have outbound permissions to Amazon Bedrock accounts. Additionally, model providers don't have access to Amazon Bedrock logs or access to customer prompts and continuations.\n",
    "\n",
    "Amazon Bedrock doesnt store or log your data in its service logs.\n",
    "\n",
    "**Q: What models are supported by Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n",
    "\n",
    "**Q: What is the difference between On-demand and Provisioned Throughput in Amazon Bedrock?**\n",
    "\n",
    "With the On-Demand mode, you only pay for what you use, with no time-based term commitments. For text generation models, you are charged for every input token processed and every output token generated. For embeddings models, you are charged for every input token processed. A token is comprised of a few characters and refers to the basic unit that a model learns to understand user input and prompt to generate results. For image generation models, you are charged for every image generated.\n",
    "\n",
    "With the Provisioned Throughput mode, you can purchase model units for a specific base or custom model. The Provisioned Throughput mode is primarily designed for large consistent inference workloads that need guaranteed throughput. Custom models can only be accessed using Provisioned Throughput. A model unit provides a certain throughput, which is measured by the maximum number of input or output tokens processed per minute. With this Provisioned Throughput pricing, charged by the hour, you have the flexibility to choose between 1-month or 6-month commitment terms.\n",
    "\n",
    "**Q: Where can I find customer references for Amazon Bedrock?**\n",
    "\n",
    "Go [here](https://aws.amazon.com/bedrock/testimonials/).\n",
    "\n",
    "**Q: Where can I find resources for prompt engineering?**\n",
    "\n",
    "[Prompt Engineering Guide](https://www.promptingguide.ai/).\n",
    "\n",
    "**Q: Where can learn more about Corrective RAG?**\n",
    "\n",
    "Go [here](https://arxiv.org/abs/2401.15884).\n",
    "\n",
    "**Q: Is LangChain mandatory to use Amazon Bedrock?**\n",
    "\n",
    "No. You can interact with Amazon Bedrock using the [Bedrock API](https://docs.aws.amazon.com/bedrock/latest/APIReference/welcome.html) or language-specific [AWS SDKs](https://aws.amazon.com/developer/tools/). \n",
    "\n",
    "**Q: How do I get started with LangChain?**\n",
    "\n",
    "Go [here](https://python.langchain.com/docs/get_started/introduction).\n",
    "\n",
    "**Q: Where can I find pricing information for the AWS services used in this notebook?**\n",
    "\n",
    "- Amazon Bedrock pricing - go [here](https://aws.amazon.com/bedrock/pricing/).\n",
    "- Amazon OpenSearch Serverless pricing - go [here](https://aws.amazon.com/opensearch-service/pricing/) and navigate to the <i>Serverless</i> section.\n",
    "- Amazon S3 pricing - go [here](https://aws.amazon.com/s3/pricing/).\n",
    "- AWS Identity and Access Management (IAM) pricing - free.\n",
    "- Amazon CloudWatch pricing - go [here](https://aws.amazon.com/cloudwatch/pricing/).\n",
    "- Amazon SageMaker Notebook instance (or) Amazon SageMaker Studio Notebook pricing - go [here](https://aws.amazon.com/sagemaker/pricing/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
